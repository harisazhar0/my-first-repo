# -*- coding: utf-8 -*-
"""Standardization analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17wsb08p3TaZQyPF0dlvt9aIXMrX75JzC

# Importance of Feature Scaling via Standardization

Feature scaling is a crucial preprocessing step in machine learning, especially when dealing with algorithms that are sensitive to the scale of input features. Standardization is one common method used for feature scaling. In this notebook, we explore what feature scaling is, why it's necessary, and delve into the mathematical formula and geometric intuition behind standardization.

## What is Feature Scaling?

Feature scaling is the process of normalizing or standardizing the range of independent variables or features in your data. It ensures that each feature contributes approximately proportionately to the final model and prevents certain features from dominating due to their larger scales.

## When Should You Use Feature Scaling?

Feature scaling is essential when:

- **The algorithm relies on distance metrics:** Algorithms like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM) compute distances between data points. If features are on different scales, these distances might be skewed, leading to biased results.
  
- **Gradient Descent Optimization:** Algorithms like Linear Regression and Neural Networks that use gradient descent for optimization converge faster when features are on a similar scale.
  
- **Regularization:** Regularization techniques (e.g., Lasso, Ridge) penalize large coefficients. Standardizing features ensures fair treatment during the regularization process.

## Mathematical Formula of Standardization

Standardization transforms the features to have the properties of a standard normal distribution with a mean (μ) of 0 and a standard deviation (σ) of 1. The standardization formula for a feature \(X\) is given by:

\[ X_{\text{standardized}} = \frac{X - \mu}{\sigma} \]

Where:
- \(X_{\text{standardized}}\) is the standardized value of feature \(X\),
- \(X\) is the original feature value,
- \(\mu\) is the mean of the feature across all samples,
- \(\sigma\) is the standard deviation of the feature across all samples.

## Geometric Intuition of Standardization

Geometrically, standardization can be visualized as a transformation that centers the data around the origin (0) and scales it, ensuring that the spread along each axis is consistent. This geometric transformation aids in better visualization and understanding of the relationships between variables.

In this notebook, we will demonstrate the effects of standardization on a dataset, explore its impact on different machine learning algorithms, and discuss scenarios where standardization might not be the best choice. Let's dive in!

## Importing data and libararies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/Social_Network_Ads.csv')
df.head(5)

x = df.iloc[:,:2]
y = df.iloc[:,2:]

"""## Splitting data"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=0)
x_train

"""## standardizing data"""

from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()

scalar.fit(x_train)
scalar.mean_

scalar.fit_transform(x_train)

x_train_scaled = scalar.transform(x_train)
x_test_scaled = scalar.transform(x_test)

## converting scaled array to dataframe
x_train_scaled = pd.DataFrame(x_train_scaled,columns =x_train.columns)
x_test_scaled = pd.DataFrame(x_test_scaled,columns =x_test.columns)
x_train_scaled

"""## Effects of Scaling"""

fig, ax = plt.subplots(1, 2, figsize=(10, 5))

ax[0].scatter(x_train['Age'], x_train['EstimatedSalary'])
ax[0].set_xlabel('before scaling')
ax[0].set_ylabel('Estimated Salary')

ax[1].scatter(x_train_scaled['Age'], x_train_scaled['EstimatedSalary'],color='green')
ax[1].set_xlabel('after scaling')
ax[1].set_ylabel('Estimated Salary')

plt.show()

"""#### as we can see after scaling both features are mean centric"""

x_train_scaled.describe()

"""#### mean is 0 and std is 1"""

fig, (ax1,ax2)= plt.subplots(ncols=2)
ax1.set_title(' Before scaling')
sns.kdeplot(x_train['Age'],ax=ax1)
sns.kdeplot(x_train['EstimatedSalary'],ax=ax1)



ax2.set_title('After scaling')
sns.kdeplot(x_train_scaled['Age'],ax=ax2)
sns.kdeplot(x_train_scaled['EstimatedSalary'],ax=ax2)
plt.show()

"""## Why feature scaling is important"""

from sklearn.linear_model import LogisticRegression
lr= LogisticRegression()
lr_scaled = LogisticRegression()

lr.fit(x_train,y_train)
lr_scaled.fit(x_train_scaled,y_train)

y_pred = lr.predict(x_test)
y_pred_scaled = lr_scaled.predict(x_test_scaled)

from sklearn.metrics import accuracy_score

print('Actual : ',accuracy_score(y_test,y_pred))
print('scaled : ',accuracy_score(y_test,y_pred_scaled))

"""## so 20% accuracy is increased after scaling"""